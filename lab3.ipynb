{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab #3 \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/basic-operations-workshop/blob/main/lab3.ipynb)\n",
    "1. Install dependencies\n",
    "2. Create a pinecone index \n",
    "3. Load public image dataset(fashion-mnist) and create vector embeddings from the dataset\n",
    "4. Create a local parquet backup of your image embeddings\n",
    "5. Insert the fashion-mnist embeddings into Pinecone\n",
    "6. Run a nearest neighbor search on a sample image that is not in the training dataset\n",
    "7. Run a nearest neighbor search on 100 random test images that are not in the training dataset\n",
    "8. Run a load test script to simulate 10 concurrent users querying the index\n",
    "9. TEARDOWN: Delete the index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install Pinecone client \n",
    "Use the following shell command to install Pinecone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"pinecone-client[grpc]\" \"python-dotenv\" \"torch\" \"torchvision\" \"pillow\" \"ftfy\" \"regex\" \"tqdm\" \"git+https://github.com/openai/clip.git\" \"datasets\" \"locust\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a pinecone index \n",
    "We will create an index that will be used to load/query a hugging face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IndexDescription(name='james-williams', metric='cosine', replicas=1, dimension=512.0, shards=1, pods=1, pod_type='s1.x1', status={'ready': True, 'state': 'Ready'}, metadata_config=None, source_collection='')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pinecone\n",
    "\n",
    "load_dotenv('.env')\n",
    "\n",
    "PINECONE_INDEX_NAME = os.environ['PINECONE_INDEX_NAME']\n",
    "PINECONE_API_KEY = os.environ['PINECONE_API_KEY']\n",
    "PINECONE_ENVIRONMENT = os.environ['PINECONE_ENVIRONMENT']\n",
    "METRIC = os.environ['METRIC']\n",
    "DIMENSIONS = int(os.environ['DIMENSIONS'])\n",
    "INDEX_NAMESPACE = os.environ['INDEX_NAMESPACE']\n",
    "\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
    "pinecone.create_index(PINECONE_INDEX_NAME, dimension=DIMENSIONS, metric=METRIC, pods=1, replicas=1, pod_type=\"s1\")\n",
    "pinecone.describe_index(PINECONE_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load public image dataset(fashion-mnist) and create vector embeddings from the dataset\n",
    "\n",
    "Use the following shell command to download the [fashion-mnist](https://huggingface.co/datasets/fashion_mnist) training dataset from Hugging Face so that we can create vector embeddings that uses a label(image class) as meta-data from this dataset. The meta-data labels mappings are:\n",
    "\n",
    "| Label  | Description |\n",
    "| ------ | ----------- |\n",
    "| 0      | T-shirt/top |\n",
    "| 1      | Trouser     |\n",
    "| 2      | Pullover    |\n",
    "| 3      | Dress       |\n",
    "| 4      | Coat        |\n",
    "| 5      | Sandal      |\n",
    "| 6      | Shirt       |\n",
    "| 7      | Sneaker     |\n",
    "| 8      | Bag         |\n",
    "| 9      | Ankle boot  |\n",
    "\n",
    "The Fashion-MNIST dataset is a dataset of Zalando's article images, consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n",
    "\n",
    "The accuracy you can achieve depends on the model and the preprocessing steps you use. Here's a rough guideline for what you might expect with some classic machine learning algorithms:\n",
    "\n",
    "1. **Random Forest:** Around 85-89% accuracy.\n",
    "2. **Support Vector Machines (SVM):** Around 85-90% accuracy, depending on kernel and hyperparameters.\n",
    "3. **k-Nearest Neighbors (k-NN):** Around 85-88% accuracy.\n",
    "4. **Logistic Regression:** Around 82-85% accuracy.\n",
    "5. **Gradient Boosting Machines (e.g., XGBoost):** Around 87-90% accuracy.\n",
    "\n",
    "Keep in mind these numbers are approximate and can vary based on the exact preprocessing, feature extraction, and hyperparameter tuning you do. In general, deep learning models, especially Convolutional Neural Networks (CNNs), tend to perform better on image classification tasks like Fashion-MNIST, potentially reaching over 90-95% accuracy.\n",
    "\n",
    "But for classic machine learning models, anything in the 85-90% range can be considered a reasonable result for the Fashion-MNIST dataset. It reflects a model that has learned something meaningful from the data but isn't necessarily state-of-the-art for this particular task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Images: 100%|██████████| 6000/6000 [03:11<00:00, 31.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm  # progress bar\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "#  Load the fashion-mnist dataset - only retrieve 6K random images (10% of total dataset)\n",
    "dataset = load_dataset(\"fashion_mnist\")['train'].shuffle(seed=42).select(range(0,6000))\n",
    "#dataset = load_dataset(\"fashion_mnist\")['train']\n",
    "\n",
    "label_descriptions = {0: \"T-shirt/top\", \n",
    "           1: \"Trouser\",\n",
    "           2: \"Pullover\",\n",
    "           3: \"Dress\",\n",
    "           4: \"Coat\",\n",
    "           5: \"Sandal\",\n",
    "           6: \"Shirt\",\n",
    "           7: \"Sneaker\",\n",
    "           8: \"Bag\",\n",
    "           9: \"Ankle boot\"}\n",
    "\n",
    "# Check to see if GPU is aviailable\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "   \n",
    "# Generate vector embeddings for each image in the dataset\n",
    "id = 0\n",
    "vectors = []\n",
    "for img in tqdm(dataset, total=dataset.num_rows, desc='Images', position=0):\n",
    "    with torch.no_grad():\n",
    "        id += 1\n",
    "        image_pp = preprocess(img['image']).unsqueeze(0).to(device)\n",
    "        image_features = model.encode_image(image_pp)\n",
    "        embedding = image_features.cpu().numpy().squeeze().tolist()\n",
    "        meta_data = {\"description\": label_descriptions[img[\"label\"]], \"timestamp\": time.time()}\n",
    "        vectors.append({'id': str(id),\n",
    "                        'values': embedding,\n",
    "                        'metadata': meta_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create a local parquet backup of your image embeddings\n",
    "\n",
    "This is good practice because generating embeddings can be expensive and time consuming when calling hosted models like OpenAI. As you can see, even locally generated embeddings are time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "df = pd.DataFrame(vectors)\n",
    "df.to_parquet('fashion-mnist-clip.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Insert the fashion-mnist embeddings into Pinecone\n",
    "\n",
    "The best way to do bulk updates is by batching the dataset. We will also use a namespace for the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60.0 [00:34<00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # progress bar\n",
    "import pinecone\n",
    "import itertools\n",
    "\n",
    "# Read Parquet file into a DataFrame\n",
    "df = pd.read_parquet('fashion-mnist-clip.parquet')\n",
    "df['values'] = df['values'].apply(lambda x: x.tolist())\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data_list = df.to_dict(orient='records')\n",
    "\n",
    "def chunks(iterable, batch_size=100):\n",
    "    \"\"\"A helper function to break an iterable into chunks of size batch_size.\"\"\"\n",
    "    it = iter(iterable)\n",
    "    chunk = tuple(itertools.islice(it, batch_size))\n",
    "    while chunk:\n",
    "        yield chunk\n",
    "        chunk = tuple(itertools.islice(it, batch_size))\n",
    "\n",
    "index = pinecone.Index(PINECONE_INDEX_NAME)\n",
    "\n",
    "# Obtain the upsert embeddings in batches of 100\n",
    "batch_size = 100\n",
    "id = 0\n",
    "for vector_batch in tqdm(chunks(data_list, batch_size=batch_size), total=(len(vectors) / batch_size)):\n",
    "   index.upsert(vector_batch, namespace=INDEX_NAMESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Run a nearest neighbor search on a sample image that is not in the training dataset\n",
    "\n",
    "Download a sneaker image file from github that we will use to run a query to see if pinecone search returns the correct description \"Sneaker\". A query returns the correct result if the most common top_k description matches the test image.\n",
    "\n",
    "You can change the top_k from 10 to 1 to 20 to see if the ANN results vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common item matching result: True\n",
      "top_k: 10 match percentage is: 70.0%\n",
      "Match miss categories: {'Ankle boot'} exepected 'Sneaker'\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "import requests\n",
    "\n",
    "# Check to see if GPU is aviailable\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device=device) \n",
    "\n",
    "def image_to_embedding():\n",
    "    \n",
    "    url = \"https://github.com/pinecone-io/basic-operations-workshop/blob/main/sneaker.jpeg?raw=true\"\n",
    "    response = requests.get(url)\n",
    "    with open(\"sneaker.jpeg\", \"wb\") as file:\n",
    "      file.write(response.content)\n",
    "    image_pp = preprocess(Image.open(\"./sneaker.jpeg\")).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "      embedding = model.encode_image(image_pp).squeeze().tolist()\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "index = pinecone.Index(PINECONE_INDEX_NAME)\n",
    "top_k = 10\n",
    "\n",
    "query_result = index.query(\n",
    "  vector = image_to_embedding(),\n",
    "  namespace=INDEX_NAMESPACE,\n",
    "  top_k=top_k,\n",
    "  include_values=False,\n",
    "  include_metadata=True\n",
    ")\n",
    "\n",
    "top_k_success = False\n",
    "match_cnt = 0\n",
    "miss_categories = set()\n",
    "\n",
    "my_list = query_result.matches\n",
    "descriptions = [entry['metadata']['description'] for entry in my_list]\n",
    "most_common_item = max(set(descriptions), key=descriptions.count)\n",
    "\n",
    "if most_common_item == \"Sneaker\":\n",
    "      top_k_success = True\n",
    "\n",
    "for match in query_result.matches:\n",
    "  if match.metadata['description'] == \"Sneaker\":\n",
    "    match_cnt += 1\n",
    "    top_k_contains = True\n",
    "  else:\n",
    "    miss_categories.add(match.metadata['description'])\n",
    "\n",
    "print(f\"Most common item matching result: {top_k_success}\")\n",
    "print(f\"top_k: {top_k} match percentage is: {match_cnt/top_k * 100}%\")\n",
    "print(f\"Match miss categories: {miss_categories} exepected 'Sneaker'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Run a nearest neighbor search on 100 random test images that are not in the training dataset\n",
    "\n",
    "Select 100 random test images. Keep in mind the model was NOT trained against these images. Obtain the percentage of pinecone queries that return the correct result in top_k. Feel free to play around with the \"top_k\" setting to see if you can increase the hit percentage. \n",
    "\n",
    "A query returns the correct result if the most common top_k description matches the test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 22.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_k success rate: 86.0%\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "import pinecone\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm \n",
    "from collections import Counter\n",
    "\n",
    "test_dataset = load_dataset(\"fashion_mnist\")['test'].shuffle().select(range(0, 100))\n",
    "#test_dataset = load_dataset(\"fashion_mnist\")['test']\n",
    "\n",
    "# Check to see if GPU is aviailable\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device=device) \n",
    "\n",
    "# Generate vector embeddings for each image in the dataset\n",
    "test_vectors = []\n",
    "for img in tqdm(test_dataset, total=test_dataset.num_rows):\n",
    "  image_pp = preprocess(img['image']).unsqueeze(0).to(device)\n",
    "  embedding = model.encode_image(image_pp).squeeze().tolist()\n",
    "    \n",
    "  test_vectors.append({'embedding': embedding,\n",
    "                        'description': label_descriptions[img[\"label\"]]})\n",
    "    \n",
    "index = pinecone.Index(PINECONE_INDEX_NAME)\n",
    "top_k = 10\n",
    "top_k_success_cnt = 0\n",
    "\n",
    "for v in test_vectors:\n",
    "\n",
    "  top_k_success = False\n",
    "\n",
    "  query_result = index.query(\n",
    "    vector = v['embedding'],\n",
    "    namespace=INDEX_NAMESPACE,\n",
    "    top_k=top_k,\n",
    "    include_values=False,\n",
    "    include_metadata=True\n",
    "  )\n",
    "\n",
    "  my_list = query_result.matches\n",
    "  descriptions = [entry['metadata']['description'] for entry in my_list]\n",
    "  most_common_item = max(set(descriptions), key=descriptions.count)\n",
    "\n",
    "  if most_common_item == v['description']:\n",
    "      top_k_success = True\n",
    "  \n",
    "  if top_k_success:\n",
    "    top_k_success_cnt += 1\n",
    "\n",
    "print(f\"top_k success rate: {top_k_success_cnt / (len(test_vectors)) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Run a load test script to simulate 10 concurrent users querying the index\n",
    "\n",
    "Locust.io is an open-source load testing tool written in Python. It allows you to define user behaviour with Python code and simulate millions of simultaneous users to bombard a system with traffic to test its resilience under heavy load. The (locustfile.py)[./locustfile.py] script re-uses the logic in step #6 to query pinecone. It has a custom event hook that denotes a failure if the top_k result set does not match the search image description. This script will likely fail with a low error rate but you can increase top_k to get a 100% pass rate. The locust summary includes P50 to P100 response time percentiles and QPS(req/s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 100 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-04 17:44:32,907] williamsj-MacBook-Pro/INFO/locust.main: Run time limit set to 60 seconds\n",
      "[2023-08-04 17:44:32,907] williamsj-MacBook-Pro/INFO/locust.main: Starting Locust 2.16.0\n",
      "[2023-08-04 17:44:32,908] williamsj-MacBook-Pro/INFO/locust.runners: Ramping to 10 users at a rate of 1.00 per second\n",
      "[2023-08-04 17:44:41,939] williamsj-MacBook-Pro/INFO/locust.runners: All users spawned: {\"PineconeUser\": 10} (10 total users)\n",
      "[2023-08-04 17:45:32,909] williamsj-MacBook-Pro/INFO/locust.main: --run-time limit reached, shutting down\n",
      "[2023-08-04 17:45:32,913] williamsj-MacBook-Pro/INFO/locust.main: Shutting down (exit code 0)\n",
      "Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
      "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
      "grpc     query_pinecone                                                                   945  117(12.38%) |     88      59     304     83 |   15.75        1.95\n",
      "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
      "         Aggregated                                                                       945  117(12.38%) |     88      59     304     83 |   15.75        1.95\n",
      "\n",
      "Response time percentiles (approximated)\n",
      "Type     Name                                                                                  50%    66%    75%    80%    90%    95%    98%    99%  99.9% 99.99%   100% # reqs\n",
      "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
      "grpc     query_pinecone                                                                         83     86     90     94    100    120    170    200    300    300    300    945\n",
      "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
      "         Aggregated                                                                             83     86     90     94    100    120    170    200    300    300    300    945\n",
      "\n",
      "Error report\n",
      "# occurrences      Error                                                                                               \n",
      "------------------|---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "117                grpc query_pinecone: 'Most common item does not match'                                              \n",
      "------------------|---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "locust -f locustfile.py --headless -u 10 -r 1 --run-time 60s --host https://pinecone.io --only-summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. TEARDOWN: Delete the index \n",
    "# WARNING: This next step will delete the PINECONE_INDEX_NAME index and all data in it. DO NOT RUN THIS UNTIL YOU ARE READY OR MANUALLY REMOVE THE INDEX INSTEAD!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PINECONE_INDEX_NAME in pinecone.list_indexes():\n",
    "    pinecone.delete_index(PINECONE_INDEX_NAME)\n",
    "    \n",
    "pinecone.list_indexes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
