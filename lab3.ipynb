{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab #3 \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/basic-operations-workshop/blob/main/lab3.ipynb)\n",
    "1. Install dependencies\n",
    "2. Create a pinecone index \n",
    "3. Load public image dataset(fashion-mnist) and create vector embeddings from the dataset\n",
    "4. Insert the fashion-mnist embeddings into Pinecone\n",
    "5. Run a nearest neighbor search on a sample image that is not in the training dataset\n",
    "6. Run a nearest neighbor search on 100 random test images that are not in the training dataset\n",
    "7. Run a load test script to simulate 10 concurrent users querying the index\n",
    "8. TEARDOWN: Delete the index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install Pinecone client \n",
    "Use the following shell command to install Pinecone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"pinecone-client[grpc]\" \"python-dotenv\" \"torch\" \"torchvision\" \"pillow\" \"ftfy\" \"regex\" \"tqdm\" \"git+https://github.com/openai/clip.git\" \"datasets\" \"locust\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a pinecone index \n",
    "We will create an index that will be used to load/query a hugging face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pinecone\n",
    "\n",
    "load_dotenv('.env')\n",
    "\n",
    "PINECONE_INDEX_NAME = os.environ['PINECONE_INDEX_NAME']\n",
    "PINECONE_API_KEY = os.environ['PINECONE_API_KEY']\n",
    "PINECONE_ENVIRONMENT = os.environ['PINECONE_ENVIRONMENT']\n",
    "METRIC = os.environ['METRIC']\n",
    "DIMENSIONS = 512\n",
    "\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
    "pinecone.create_index(PINECONE_INDEX_NAME, dimension=DIMENSIONS, metric=METRIC, pods=1, replicas=1, pod_type=\"s1\")\n",
    "pinecone.describe_index(PINECONE_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load public image dataset(fashion-mnist) and create vector embeddings from the dataset\n",
    "\n",
    "Use the following shell command to download the [fashion-mnist](https://huggingface.co/datasets/fashion_mnist) training dataset from Hugging Face so that we can create vector embeddings that uses a label(image class) as meta-data from this dataset. The meta-data labels mappings are:\n",
    "\n",
    "| Label  | Description |\n",
    "| ------ | ----------- |\n",
    "| 0      | T-shirt/top |\n",
    "| 1      | Trouser     |\n",
    "| 2      | Pullover    |\n",
    "| 3      | Dress       |\n",
    "| 4      | Coat        |\n",
    "| 5      | Sandal      |\n",
    "| 6      | Shirt       |\n",
    "| 7      | Sneaker     |\n",
    "| 8      | Bag         |\n",
    "| 9      | Ankle boot  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm  # progress bar\n",
    "import torch\n",
    "import clip\n",
    "import time\n",
    "\n",
    "#  Load the fashion-mnist dataset - only retrieve 6000 random images (10% of total dataset)\n",
    "dataset = load_dataset(\"fashion_mnist\")['train'].shuffle(seed=42).select(range(0,6000))\n",
    "\n",
    "label_descriptions = {0: \"T-shirt/top\", \n",
    "           1: \"Trouser\",\n",
    "           2: \"Pullover\",\n",
    "           3: \"Dress\",\n",
    "           4: \"Coat\",\n",
    "           5: \"Sandal\",\n",
    "           6: \"Shirt\",\n",
    "           7: \"Sneaker\",\n",
    "           8: \"Bag\",\n",
    "           9: \"Ankle boot\"}\n",
    "\n",
    "# Check to see if GPU is aviailable\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "\n",
    "# Generate vector embeddings for each image in the dataset\n",
    "id = 0\n",
    "vectors = []\n",
    "for image in tqdm(dataset, total=dataset.num_rows):\n",
    "    with torch.no_grad():\n",
    "        image_pp = preprocess(image['image']).unsqueeze(0).to(device)\n",
    "        image_features = model.encode_image(image_pp)\n",
    "        embedding_numpy = image_features.cpu().numpy().squeeze().tolist()\n",
    "        id += 1\n",
    "        meta_data = {\"description\": label_descriptions[image[\"label\"]], \"timestamp\": time.time()}\n",
    "        vectors.append({'id': str(id),\n",
    "                        'values': embedding_numpy,\n",
    "                        'metadata': meta_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Insert the fashion-mnist embeddings into Pinecone\n",
    "\n",
    "The best way to do bulk updates is by batching the dataset. We will also use a namespace for the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # progress bar\n",
    "import pinecone\n",
    "import itertools\n",
    "\n",
    "def chunks(iterable, batch_size=100):\n",
    "    \"\"\"A helper function to break an iterable into chunks of size batch_size.\"\"\"\n",
    "    it = iter(iterable)\n",
    "    chunk = tuple(itertools.islice(it, batch_size))\n",
    "    while chunk:\n",
    "        yield chunk\n",
    "        chunk = tuple(itertools.islice(it, batch_size))\n",
    "\n",
    "index = pinecone.Index(PINECONE_INDEX_NAME)\n",
    "\n",
    "# Obtain the upsert embeddings in batches of 100\n",
    "batch_size = 100\n",
    "id = 0\n",
    "for vector_batch in tqdm(chunks(vectors, batch_size=batch_size), total=(len(vectors) / batch_size)):\n",
    "   index.upsert(vector_batch, namespace=\"fashion-mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run a nearest neighbor search on a sample image that is not in the training dataset\n",
    "\n",
    "Download a sneaker image file from github that we will use to run a query to see if pinecone search returns the correct description \"Sneaker\". \n",
    "You can change the top_k from 1 to 10 to 100 to see how the ANN results vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "import requests\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "\n",
    "def image_to_embedding():\n",
    "    \n",
    "    url = \"https://github.com/pinecone-io/basic-operations-workshop/blob/main/sneaker.png?raw=true\"\n",
    "    response = requests.get(url)\n",
    "    with open(\"sneaker.png\", \"wb\") as file:\n",
    "      file.write(response.content)\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = preprocess(Image.open(\"./sneaker.png\")).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate the image features\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "    return image_features\n",
    "\n",
    "embedding = image_to_embedding().cpu().numpy().squeeze().tolist()\n",
    "\n",
    "index = pinecone.Index(PINECONE_INDEX_NAME)\n",
    "top_k = 10\n",
    "\n",
    "query_result = index.query(\n",
    "  vector = embedding,\n",
    "  namespace=\"fashion-mnist\",\n",
    "  top_k=top_k,\n",
    "  include_values=False,\n",
    "  include_metadata=True\n",
    ")\n",
    "\n",
    "top_k_contains = False\n",
    "match_cnt = 0\n",
    "miss_categories = set()\n",
    "\n",
    "for match in query_result.matches:\n",
    "  if match.metadata['description'] == \"Sneaker\":\n",
    "    match_cnt += 1\n",
    "    top_k_contains = True\n",
    "  else:\n",
    "    miss_categories.add(match.metadata['description'])\n",
    "\n",
    "print(f\"top_k contains matching result: {top_k_contains}\")\n",
    "print(f\"top_k: {top_k} match percentage is: {match_cnt/top_k * 100}%\")\n",
    "print(f\"Match miss categories: {miss_categories} exepected 'Sneaker'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Run a nearest neighbor search on 100 random test images that are not in the training dataset\n",
    "\n",
    "Select 100 random test images. Keep in mind the model was NOT trained against these images. Obtain the percentage of pinecone queries that return the correct result in top_k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import pinecone\n",
    "from datasets import load_dataset\n",
    "import tqdm\n",
    "\n",
    "test_dataset = load_dataset(\"fashion_mnist\")['test'].shuffle(seed=42).select(range(0, 100))\n",
    "\n",
    "# Check to see if GPU is aviailable\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "\n",
    "# Generate vector embeddings for each image in the dataset\n",
    "test_vectors = []\n",
    "for image in tqdm(test_dataset, total=test_dataset.num_rows):\n",
    "    with torch.no_grad():\n",
    "        image_pp = preprocess(image['image']).unsqueeze(0).to(device)\n",
    "        image_features = model.encode_image(image_pp)\n",
    "        embedding_numpy = image_features.cpu().numpy().squeeze().tolist()\n",
    "        id += 1\n",
    "        test_vectors.append({'embedding': embedding_numpy,\n",
    "                        'description': label_descriptions[image[\"label\"]]})\n",
    "\n",
    "index = pinecone.Index(PINECONE_INDEX_NAME)\n",
    "top_k = 10\n",
    "top_k_contains_cnt = 0\n",
    "\n",
    "for v in test_vectors:\n",
    "\n",
    "  top_k_contains = False\n",
    "\n",
    "  query_result = index.query(\n",
    "    vector = v['embedding'],\n",
    "    namespace=\"fashion-mnist\",\n",
    "    top_k=top_k,\n",
    "    include_values=False,\n",
    "    include_metadata=True\n",
    "  )\n",
    "  \n",
    "  for match in query_result.matches:\n",
    "    if match.metadata['description'] == v['description']:\n",
    "      top_k_contains = True\n",
    "\n",
    "  if top_k_contains:\n",
    "    top_k_contains_cnt += 1\n",
    "\n",
    "print(f\"top_k contains matching result: {top_k_contains_cnt / (len(test_vectors)) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Run a load test script to simulate 10 concurrent users querying the index\n",
    "\n",
    "Locust.io is an open-source load testing tool written in Python. It allows you to define user behaviour with Python code and simulate millions of simultaneous users to bombard a system with traffic to test its resilience under heavy load. The (locustfile.py)[./locustfile.py] script re-uses the logic in step #6 to query pinecone. It has a custom event hook that denotes a failure if the top_k result set does not match the search image description. This script will likely fail with a low error rate but you can increase top_k to get a 100% pass rate. The locust summary includes P50 to P100 response time percentiles and QPS(req/s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "locust -f locustfile.py --headless -u 10 -r 1 --run-time 60s --host https://pinecone.io --only-summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. TEARDOWN: Delete the index \n",
    "# WARNING: This next step will delete the PINECONE_INDEX_NAME index and all data in it. DO NOT RUN THIS UNTIL YOU ARE READY OR MANUALLY REMOVE THE INDEX INSTEAD!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PINECONE_INDEX_NAME in pinecone.list_indexes():\n",
    "    pinecone.delete_index(PINECONE_INDEX_NAME)\n",
    "    \n",
    "pinecone.list_indexes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
